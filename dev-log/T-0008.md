# T-0008: Launch ICE training pod

**Status**: DONE
**Phase**: VERIFYING
**After**: T-0006
**Before**: T-0002

## Context

With the ICE GPU cluster tools now available in `ice/`, we need to launch a training pod before we can install GPU-dependent libraries and run training. This task provisions the remote GPU environment.

## Acceptance Criteria

- [x] Training pod is running on ICE cluster
- [x] Pod has 1x nvidia-gtx-2080ti GPU allocated (2x had scheduling issues)
- [x] Code is synced to the pod via `ice/sync.sh`
- [x] Can shell into pod and run `nvidia-smi`

## Verification Plan

### Tests to Create
| Test | Proves | Status |
|------|--------|--------|
| `kubectl get pod urm-train` | Pod is running | - |
| `nvidia-smi` on pod | GPUs allocated | - |

### Regression Check
```
./ice/status.sh
```

### Manual Verification
- [ ] Shell into pod and confirm environment is ready

## Approach

1. Run `./ice/gpus.sh` to confirm GPU availability
2. Run `./ice/train.sh urm-train nvidia-gtx-2080ti 2` to launch pod
3. Run `./ice/sync.sh urm-train` to sync code
4. Run `./ice/shell.sh urm-train` and verify with `nvidia-smi`

## Implementation Notes

### PLANNING

- Cluster connected: icekube
- GPU availability checked: 2080ti nodes available
- First attempt with 2 GPUs failed scheduling (memory constraints)
- Proceeded with 1 GPU which succeeded

### TESTING_RED

### IMPLEMENTING

1. Ran `./ice/train.sh urm-train nvidia-gtx-2080ti 1`
2. Pod created and scheduled on p02r10srv05
3. Container image pulled: pytorch/pytorch:2.1.0-cuda12.1-cudnn8-devel
4. Ran `./ice/sync.sh urm-train` - code and data synced

### TESTING_GREEN

### REFACTORING

### DEBUGGING

## Obstacles

## Evidence

```
$ kubectl get pods -n aic
NAME                   READY   STATUS    RESTARTS   AGE
urm-urm-train          1/1     Running   0          8m32s

$ kubectl exec urm-urm-train -n aic -- nvidia-smi --query-gpu=name,memory.total --format=csv
name, memory.total [MiB]
NVIDIA GeForce RTX 2080 Ti, 11264 MiB
```

## Outcome

Pod `urm-urm-train` running on ICE cluster with 1x RTX 2080 Ti. Code synced and ready for next steps (PyTorch install, flash-attn, etc.).
