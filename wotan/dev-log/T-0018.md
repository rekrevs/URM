# T-0018: Prepare Sudoku dataset

**Status**: DONE
**Phase**: DONE
**After**: —
**Before**: —

## Context

Prepare the Sudoku dataset for training as an alternative/complementary benchmark to ARC-AGI. Sudoku is a well-defined constraint satisfaction problem that may provide faster feedback on whether the URM architecture is working correctly.

Source: HuggingFace `sapientinc/sudoku-extreme`

## Acceptance Criteria

- [x] data/sudoku-extreme-1k-aug-1000/ directory exists with train/test splits
- [x] Dataset can be loaded by pretrain.py

## Verification Plan

### Tests to Create
| Test | Proves | Status |
|------|--------|--------|
| Manual: ls data/sudoku-extreme-1k-aug-1000/ | Dataset files exist | - |

### Regression Check
```
ls -la data/sudoku-extreme-1k-aug-1000/
```

### Manual Verification
- [ ] Verify dataset structure matches expected format

## Approach

Run the build_sudoku_dataset.py script:
```bash
python -m data.build_sudoku_dataset \
  --output-dir data/sudoku-extreme-1k-aug-1000 \
  --subsample-size 1000 \
  --num-aug 1000
```

## Implementation Notes

### PLANNING

### TESTING_RED

### IMPLEMENTING

Successfully ran the dataset generation command:
```bash
python -m data.build_sudoku_dataset \
  --output-dir data/sudoku-extreme-1k-aug-1000 \
  --subsample-size 1000 \
  --num-aug 1000
```

The script completed in approximately 32 seconds, processing 1000 base puzzles and generating augmented versions.

### TESTING_GREEN

### REFACTORING

### DEBUGGING

## Obstacles

None encountered. The dataset generation completed successfully on the first attempt.

## Evidence

Dataset directory structure:
```
data/sudoku-extreme-1k-aug-1000/
├── identifiers.json
├── train/
│   ├── all__group_indices.npy
│   ├── all__inputs.npy (648MB)
│   ├── all__labels.npy (648MB)
│   ├── all__puzzle_identifiers.npy
│   ├── all__puzzle_indices.npy
│   └── dataset.json
└── test/
    ├── all__group_indices.npy
    ├── all__inputs.npy (34MB)
    ├── all__labels.npy (34MB)
    ├── all__puzzle_identifiers.npy
    ├── all__puzzle_indices.npy
    └── dataset.json
```

Total dataset size: 1.3GB

## Outcome

Successfully prepared Sudoku dataset with the following characteristics:

**Training Set:**
- 1,001,000 examples (1000 base puzzles × 1000 augmentations + 1000 originals)
- Shape: (1001000, 81) - representing 9×9 Sudoku grids flattened
- Vocab size: 11 (digits 0-9, plus padding)
- Sequence length: 81 (9×9 grid)
- Total groups: 1000

**Test Set:**
- 422,786 examples from HuggingFace sapientinc/sudoku-extreme
- Shape: (422786, 81)
- Same vocab size and sequence length as training

The dataset is now ready for training with pretrain.py using the Sudoku configuration.
