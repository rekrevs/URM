# T-0013: Fix evaluator single-GPU distributed bug

**Status**: DONE
**Phase**: VERIFIED
**After**: —
**Before**: —

## Context

The ARC evaluator (`evaluators/arc.py:113`) crashes on single-GPU runs because it calls `dist.gather_object()` without checking if distributed training is initialized. Training completes successfully but the final metrics gathering fails.

Error:
```
ValueError: Default process group has not been initialized, please make sure to call init_process_group.
```

## Acceptance Criteria

- [ ] `dist.gather_object()` is guarded by `dist.is_initialized()` check
- [ ] Single-GPU training runs complete without crash
- [ ] Multi-GPU distributed training still works correctly

## Verification Plan

### Tests to Create
| Test | Proves | Status |
|------|--------|--------|
| Manual: Run single-GPU training | No crash at evaluation | - |

### Regression Check
```
python -c "from evaluators.arc import ARCEvaluator; print('OK')"
```

### Manual Verification
- [ ] Run training on ICE pod with single GPU

## Approach

Guard `dist.gather_object()` call with `dist.is_initialized()` check. For single-GPU mode, use local data directly without distributed gather.

## Implementation Notes

### PLANNING
- Identified fix location: `evaluators/arc.py:110-117`
- Simple guard: check `dist.is_initialized()` before gather
- Single-GPU fallback: wrap local data in list to match expected format

### IMPLEMENTING
Changed `result()` method:
```python
if dist.is_initialized():
    global_hmap_preds = [None for _ in range(world_size)] if rank == 0 else None
    dist.gather_object((self._local_hmap, self._local_preds), global_hmap_preds, dst=0, group=group)
else:
    # Single-GPU mode: use local data directly
    global_hmap_preds = [(self._local_hmap, self._local_preds)]
```

### TESTING_GREEN
- Import test passes locally
- Import test passes on ICE pod
- `dist.is_initialized()` correctly returns False in single-GPU mode

## Evidence

```
$ python -c "from evaluators.arc import ARC; import torch.distributed as dist; print('dist.is_initialized():', dist.is_initialized())"
dist.is_initialized(): False
OK
```

## Outcome

Fixed single-GPU evaluator crash by guarding distributed gather with initialization check.
