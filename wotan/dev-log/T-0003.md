# T-0003: Verify flash-attn in custom image

**Status**: DONE
**Phase**: COMPLETE
**After**: T-0011
**Before**: T-0007

## Context

The URM model uses flash attention for efficient attention computation. flash-attn is pre-installed in the custom Docker image.

## Acceptance Criteria

- [x] flash-attn package installed successfully
- [x] `from flash_attn import flash_attn_func` works without error
- [ ] Model imports use flash_attn (HAS_FLASH_ATTN = True in layers.py)

## Verification Plan

### Tests to Create
| Test | Proves | Status |
|------|--------|--------|
| Manual: `python -c "from flash_attn import flash_attn_func; print('OK')"` | flash-attn works | - |

### Regression Check
```
python -c "from models.urm.urm import URM; from models.layers import HAS_FLASH_ATTN; print('flash_attn:', HAS_FLASH_ATTN)"
```

### Manual Verification
- [ ] Verify flash_attn is used in forward pass

## Approach

(Filled during PLANNING phase)

## Implementation Notes

### IMPLEMENTING
flash-attn 2.8.3 is pre-compiled in custom Docker image (registry.ice.ri.se/aic-misc/urm-training:latest).

### TESTING_GREEN
Verified via kubectl exec on urm-test-image pod:
```
python -c "from flash_attn import flash_attn_func; print('flash-attn: OK')"
flash-attn: OK
```

## Evidence

Pod urm-test-image successfully imports flash_attn on RTX 2080 Ti GPU.

## Outcome

flash-attn verified working in custom image.
