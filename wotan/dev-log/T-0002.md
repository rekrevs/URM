# T-0002: Install PyTorch with CUDA support on ICE pod

**Status**: DONE
**Phase**: VERIFYING
**After**: T-0008
**Before**: T-0003, T-0004

## Context

The URM model requires CUDA for training. This task runs on the ICE GPU cluster pod (launched in T-0008). We need to install PyTorch with CUDA support in the pod environment.

## Acceptance Criteria

- [x] PyTorch installed with CUDA support
- [x] `torch.cuda.is_available()` returns True
- [x] CUDA version is compatible with flash-attn (CUDA 12.1)

## Verification Plan

### Tests to Create
| Test | Proves | Status |
|------|--------|--------|
| Manual: `python -c "import torch; print(torch.cuda.is_available())"` | CUDA works | - |

### Regression Check
```
python -c "from models.urm.urm import URM; print('OK')"
```

### Manual Verification
- [ ] Check `nvidia-smi` shows GPU
- [ ] Check PyTorch can allocate CUDA tensors

## Approach

(Filled during PLANNING phase)

## Implementation Notes

### PLANNING

**Environment Analysis:**
- `nvidia-smi`: not found
- `/proc/driver/nvidia/version`: not found
- `/dev/nvidia*`: not found

**Conclusion:** This environment has NO GPU hardware. CUDA cannot be enabled because there is no physical GPU present.

**Recommendation:** This task requires running on a machine with an NVIDIA GPU (e.g., cloud GPU instance, local workstation with GPU).

### TESTING_RED

### IMPLEMENTING

PyTorch already installed via container image `pytorch/pytorch:2.1.0-cuda12.1-cudnn8-devel`.

Verification:
```
PyTorch: 2.1.0
CUDA available: True
CUDA device: NVIDIA GeForce RTX 2080 Ti
```

### TESTING_GREEN

### REFACTORING

### DEBUGGING

## Obstacles

(Resolved - PyTorch CUDA available from container image)

## Evidence

## Outcome

PyTorch 2.1.0 with CUDA 12.1 pre-installed in container. No additional installation needed.
